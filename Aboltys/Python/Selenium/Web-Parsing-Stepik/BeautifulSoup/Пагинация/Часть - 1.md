Пагинация - это ключевой элемент веб-дизайна, который не только обеспечивает удобство навигации, но и позволяет нам, разработчикам и аналитикам, быстро определить общее количество доступных страниц.

Пагинация (англ. [Pagination](https://en.wikipedia.org/wiki/Pagination)) встречается повсюду, и нам приходится сталкиваться с ней почти всегда при написании парсера. В этом разделе мы рассмотрим пагинацию, научимся проходить по ней в цикле и извлекать информацию с каждой страницы.

В нашем [тренажере](http://parsinger.ru/legal/html/index1_page_1.html) пагинация также присутствует. На скриншоте выделены две области. В верхней области представлена ссылка, которая заканчивается числом. Как правило, это число соответствует номеру страницы. Мы можем модифицировать эту ссылку, выполняя запросы на каждой итерации. Во второй области представлены кнопки пагинации.

В нашем тренажере используется простая пагинация, включающая всего 4 страницы, но этого вполне достаточно для понимания механизма работы.

![](https://ucarecdn.com/7be27ac9-a7e5-4c5c-bf58-fba1304e0957/)

Итак, приступим к задаче. Первым делом, нам нужно определить, сколько всего страниц представлено на сайте. Эта информация обычно содержится в блоке пагинации, который позволяет пользователю переключаться между разными страницами контента.

Чтобы понять, как работает пагинация и как из неё извлечь нужную информацию, давайте рассмотрим HTML-код этого блока. Особое внимание стоит уделить структуре этого кода, а также атрибутам, которые могут нам помочь в дальнейшем.

![](https://ucarecdn.com/1f30ecbd-795e-46c7-b987-5cbefcc6689f/)

В данном случае мы видим, что пагинация реализована с помощью блока `<div>` с классом `'pagen'`. Внутри этого блока находятся четыре ссылки, представленные тегами `<a>`.

Каждая из этих ссылок ведет на определенную страницу контента. Для извлечения этих ссылок мы можем использовать методы `.find()` и `.find_all()`, с которыми мы уже знакомы из предыдущих уроков. Эти методы позволят нам легко найти и собрать все необходимые нам ссылки из блока пагинации.

Кроме того, в контексте пагинации часто важно знать, какая из страниц является последней. В нашем случае это значение последнего элемента. Зная номер последней страницы, мы можем определить, сколько итераций нам потребуется для обработки всего контента, и соответственно настроить наш парсер на работу с каждой из страниц.

```python
from bs4 import BeautifulSoup
import requests

# Задаем URL-адрес веб-страницы для парсинга
url = 'http://parsinger.ru/html/index1_page_3.html'

# Отправляем GET-запрос к указанной странице
response = requests.get(url=url)

# Устанавливаем кодировку ответа сервера в UTF-8 для корректного отображения текста на кириллице
response.encoding = 'utf-8'

# Преобразуем текст ответа сервера в объект BeautifulSoup с использованием парсера 'lxml'
soup = BeautifulSoup(response.text, 'lxml')

# Ищем блок пагинации (элемент <div> с классом 'pagen') на странице, 
# затем извлекаем из него все вложенные ссылки (элементы <a>)
pagen = soup.find('div', class_='pagen').find_all('a')

# Выводим на экран список найденных ссылок
print(pagen)
```

Вывод:

```javascript
[<a href="index1_page_1.html">1</a>, <a href="index1_page_2.html">2</a>, <a href="index1_page_3.html">3</a>, <a href="index1_page_4.html">4</a>]
```

В результате выполнения кода мы получили список HTML-тегов `<a>`. Однако наша цель — извлечь из них атрибут `href`, который содержит ссылку, а также текстовое содержимое каждого тега.

Для более компактного и эффективного решения этой задачи можно использовать механизм list comprehension. Это позволяет создавать списки на лету, применяя к каждому элементу определенное выражение.

Пример использования list comprehension:

```bash
pagen = [link['href'] for link in soup.find('div', class_='pagen').find_all('a')]
```

В результате выполнения этого кода мы получим следующий список ссылок:

```css
['index1_page_1.html', 'index1_page_2.html', 'index1_page_3.html', 'index1_page_4.html']
```

Если вы не знакомы с list comprehension, можно представить его аналог в виде обычного цикла, результат будет тот же:

```bash
pagen = []
for link in soup.find('div', class_='pagen').find_all('a'):
    pagen.append(link['href'])
```

В процессе веб-скрапинга очень важно уметь извлекать различные атрибуты из HTML-тегов. Например, атрибут `href` у тега `<a>` указывает на URL-адрес, на который ведет данная ссылка. По аналогии, у тега `<img>` есть атрибут `src`, который содержит URL-адрес изображения.

Однако просто извлечение этих атрибутов не всегда дает нам полноценные ссылки. Во многих случаях, особенно при работе с пагинацией, мы получаем относительные пути к файлам или изображениям. Это значит, что перед тем, как использовать их, нам нужно преобразовать их в полные URL-адреса.

В Python для этого можно использовать f-строки — это способ форматирования строк, который позволяет вставлять значения переменных прямо в текст. Например, если у нас есть базовый URL-адрес и относительный путь, мы можем объединить их следующим образом:

За схемой далеко ходить не нужно, стоит обратить внимание на ссылку.

[**https://parsinger.ru/html/**index1_page_1.html](https://parsinger.ru/html/index1_page_1.html)

Виды путей.

```ini
base_url = 'http://example.com/'
relative_path = 'path/to/file.html'
full_url = f'{base_url}{relative_path}'
```

Теперь, когда мы понимаем, как формируются URL-адреса на сайте, мы можем разработать схему для их генерации. Основная идея заключается в том, чтобы определить общую структуру URL-адресов и использовать ее для создания полных ссылок на нужные нам страницы или ресурсы.

Чтобы понять эту структуру, нам не нужно искать дополнительную информацию в других источниках. Все, что нам нужно, уже есть в адресной строке браузера. Анализируя ее, мы можем выявить закономерности и использовать их для создания наших собственных ссылок.

Создадим переменную `schema` и сохраним в нее первую часть ссылки. И в цикле на каждой итерации мы будем склеивать обе части, чтобы получить корректную ссылку.

```python
from bs4 import BeautifulSoup
import requests

# Задаем URL-адрес веб-страницы для парсинга
url = 'http://parsinger.ru/html/index1_page_3.html'

# Отправляем GET-запрос к указанной странице
response = requests.get(url=url)

# Устанавливаем кодировку ответа сервера в UTF-8 для корректного отображения текста на кириллице
response.encoding = 'utf-8'

# Преобразуем текст ответа сервера в объект BeautifulSoup с использованием парсера 'lxml'
soup = BeautifulSoup(response.text, 'lxml')

# Ищем блок пагинации и извлекаем все вложенные ссылки
pagen = soup.find('div', class_='pagen').find_all('a')

# Инициализируем список для хранения абсолютных URL-адресов
list_link = []

# Задаем схему URL-адреса, которая будет использоваться для преобразования относительных путей в абсолютные URL
schema = 'http://parsinger.ru/html/'

# Цикл по всем найденным ссылкам для преобразования их в абсолютные URL-адреса
for link in pagen:
    list_link.append(f"{schema}{link['href']}")

# Выводим на экран список абсолютных URL-адресов
print(list_link)
```

Вывод:

```css
['http://parsinger.ru/html/index1_page_1.html', 'http://parsinger.ru/html/index1_page_2.html', 'http://parsinger.ru/html/index1_page_3.html', 'http://parsinger.ru/html/index1_page_4.html']
```

Отлично, всё получилось. 

То же самое, но с применением **list comprehension:**

```python
from bs4 import BeautifulSoup
import requests

url = 'http://parsinger.ru/html/index1_page_3.html'
response = requests.get(url=url)
response.encoding = 'utf-8'
soup = BeautifulSoup(response.text, 'lxml')
schema = 'http://parsinger.ru/html/'
pagen = [f"{schema}{link['href']}" for link in soup.find('div', class_='pagen').find_all('a')]

print(pagen)
```

Вывод:

```css
['http://parsinger.ru/html/index1_page_1.html', 'http://parsinger.ru/html/index1_page_2.html', 'http://parsinger.ru/html/index1_page_3.html', 'http://parsinger.ru/html/index1_page_4.html']
```