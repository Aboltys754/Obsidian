Иногда на сайтах все ссылки на страницы (как кнопки "вперед" или "назад" в интернет-магазинах) не показываются сразу. Это как книга, где оглавление показывает только часть глав.

Допустим, вы хотите собрать информацию со всех страниц сайта, но у вас есть ссылка только на одну или несколько из них. Что делать? Нужно самому "собрать" эти ссылки.

Посмотрим на пример с сайтом www.ozon.ru. Если вы присмотрите к адресам страниц в интернет-магазинах, то заметите, что они часто похожи между собой, и отличаются, например, номером в конце. Этот номер и показывает, какая это страница по счету.

`https://www.ozon.ru/category/knigi-16500/?category_was_predicted=true&**page=2**`

Зная это, вы можете написать код, который сам будет генерировать ссылки на все страницы, даже если на сайте они не отображаются. Это как если бы ты знал, что после главы 1 идет глава 2, потом 3 и так далее, и просто листал книгу дальше.

Таким образом, даже если сайт не показывает все ссылки сразу, вы все равно сможете получить информацию со всех страниц.

Мы можем использовать эту особенность, чтобы сгенерировать себе новые ссылки на все страницы.

Пример: если у вас есть ссылка `http://сайт.ру/страница_1`, то следующая страница, скорее всего, будет `http://сайт.ру/страница_2`, потом `страница_3` и так далее.

В Python есть удобная штука, которая называется [f-строка](https://shultais.education/blog/python-f-strings). Это способ создавать строки, вставляя в них значения переменных. Просто представьте, что это как конструктор: вы берёте основную часть строки и добавляешь к ней то, что нужно изменить (в нашем случае — номер страницы).

```bash
link = []
for i in range(1, 101):
    link.append(f'https://www.ozon.ru/category/knigi-16500/?category_was_predicted=true&page={i}')
print(link)
```

Вывод:

```rust
['https://www.ozon.ru/category/knigi-16500/?category_was_predicted=true&page=1', 
 'https://www.ozon.ru/category/knigi-16500/?category_was_predicted=true&page=2', 
    ...
 'https://www.ozon.ru/category/knigi-16500/?category_was_predicted=true&page=100']
```

Таким способом мы можем сразу создать 100 ссылок для парсинга. Но проблема в том, что мы заранее не можем быть уверены в количестве страниц на сайте. Сегодня их может быть 100, а завтра — 105 или 95. Если мы не учтем это, наш парсер может столкнуться с ошибкой или пропустить новые страницы.

По этой причине перед созданием ссылок нам стоит узнать, какое последнее значение находится в пагинации.

За основу примера возьмём наш [тренажер](http://parsinger.ru/html/index1_page_1.html).

```python
from bs4 import BeautifulSoup
import requests

url = 'http://parsinger.ru/html/index1_page_3.html'
response = requests.get(url=url)
response.encoding = 'utf-8'
soup = BeautifulSoup(response.text, 'lxml')
schema = 'http://parsinger.ru/html/'
pagen = [link.text for link in soup.find('div', class_='pagen').find_all('a')][-1]

print(pagen)

>>> 4
```

Мы применили индексацию **[-1]**, чтобы получить последний элемент списка, в котором хранился весь список значений пагинации.

Выполните код выше у себя в терминале, посмотрите на результат, без метода `.text` и без индекса. Попытайтесь понять, почему мы используем этот подход. 

![](https://ucarecdn.com/b01ebed8-74b9-4f86-a90e-cfae1fb04ffa/)

Если мы не узнаем последнее значение в пагинации, то наша задача — проверять каждую страницу на наличие контента. Другими словами, мы будем генерировать ссылки и проверять их. Если сервер отвечает кодом 200 (что значит "Всё хорошо, страница найдена"), мы продолжаем парсинг собирая нужную информацию. Если же ответ другой (например, 404 — "Страница не найдена"), значит, такой страницы нет, и нам пора остановиться.

Давайте взглянем на код:

```python
# Наглядный псевокод

from bs4 import BeautifulSoup
import requests

base_url = "http://сайт.ру/страница_"
num_page = 1  # начнем с первой страницы

while True:
    url = f"{base_url}{num_page }"
    response = requests.get(url)
    
    # Если статус ответа 200, продолжаем парсинг
    if response.status_code == 200:
        # Здесь ваш код для парсинга содержимого страницы
        # ...
        num_page += 1
    else:
        # Если статус ответа не 200, завершаем цикл
        break
```

Этот код будет переходить на следующую страницу, пока не встретит страницу, которой нет (или другую ошибку). Таким образом, он автоматически определит, сколько страниц нужно обработать.